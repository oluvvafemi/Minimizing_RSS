{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::opts_chunk$set(echo = TRUE)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Theory\n",
                "## Regression Tasks in Machine Learning.\n",
                "\n",
                "Regression refers to tasks when you have to predict the value of a quantitative variable. A typical regression task would be - given the features of a laptop such as: size of RAM, screen resolution, manufacturer, CPU, and GPU, predict the price.\n",
                "\n",
                "There are several techniques for regression, but here we will use linear models. You will see how to implement linear models for regression in Python, and also diagnostics and evaluation of linear models for regression.\n",
                "\n",
                "\n",
                "## Linear Regression.\n",
                "\n",
                "Linear Regression is a simple approach that assumes there is a linear relationship between the features and the response variable. In cases where you have to predict the response based on one single feature it is called *simple linear regression* $y = \\beta_0 + \\beta_1x$. When there are more than one feature it is called a *multiple linear regression* $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +...+\\beta_nx_n$.  \n",
                "\n",
                "Our goal is to estimate the values of $\\beta$. \n",
                "<!-- [//]: # (If we had a data that contains different values of ) -->\n",
                "\n",
                "## Estimating $\\beta$ \n",
                "\n",
                "The most common way of fitting a linear regression model is the least squares method. There are different approaches for estimating the coefficients using least squares: the first is a closed-form or analytical approach, and the other is using iterative optimization methods. We will discuss the advantages of both methods and how to implement it.\n",
                "\n",
                "## Closed-form estimation\n",
                "\n",
                "With this approach we seek $\\beta$ that minimizes the *Residual Sum of Squares*: \n",
                "$$RSS(\\beta) = \\sum_{j=1}^{m} (\\hat{y}_j - x_j^T\\beta)^2 $$ \n",
                "<!-- We set the gradient of RSS = 0 : $\\nabla RSS(\\beta) = -2X^T(y- X\\beta)$  -->\n",
                "where the closed-form solution of $\\hat{\\beta}$ is given by: $$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
                "\n",
                "## Using iterative methods\n",
                "\n",
                "Iterative methods are mathematical procedures that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n-th approximation is derived from the previous ones.^[Iterative method https://en.wikipedia.org/wiki/Iterative_method] \n",
                "\n",
                "Least-squares can be viewed as an optimization problem where we have to minimize: \n",
                "$$\\min \\quad RSS(\\beta) = \\|y - X\\beta\\|^2 = \\sum_{j=1}^{m} (\\hat{y}_j - x_j^T\\beta)^2$$\n",
                "We will look at only descent methods. This means that $RSS(\\beta^{(k+1)})<RSS(\\beta^{(k)})$ until we reach convergence, i.e the $RSS$ at the current iteration is lower than $RSS$ at the previous iteration until some stopping criteria is satisfied. There will be another blog post exploring in details these optimization methods.\n",
                "\n",
                "\n",
                "## Gradient Descent\n",
                "\n",
                "The gradient descent procedure for minimizing RSS is presented below.\n",
                "\n",
                "*****\n",
                "<!-- > **Algorithm**   -->\n",
                "<!-- >       initial guess of $\\beta$ -->\n",
                "<!-- >       **repeat** -->\n",
                "<!-- >           1. $\\Delta\\beta := -\\nabla RSS(\\beta)$ -->\n",
                "<!-- >  -->\n",
                "<!-- >  -->\n",
                "<!-- > Here's some example code: -->\n",
                "<!-- >  -->\n",
                "<!-- >     return shell_exec(\"echo $input | $markdown_script\"); -->\n",
                "\n",
                "\n",
                " <!-- **Algorithm**   -->\n",
                " <!--       initial guess of $\\beta$ -->\n",
                " <!--      **repeat** -->\n",
                " <!--          1. $\\Delta\\beta := -\\nabla RSS(\\beta)$ -->\n",
                "**Algorithm**  \n",
                "  initial guess of $\\beta$  \n",
                "  **repeat**  \n",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. $\\Delta\\beta := -\\nabla RSS(\\beta)$  \n",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. update  $\\beta := \\beta + t\\Delta\\beta$  \n",
                "  **until** convergence\n",
                "\n",
                "*****\n",
                "$t$ - fixed stepsize, $\\nabla RSS(\\beta) = -2X^T(y-X\\beta)$ - derivative of RSS, $\\Delta\\beta$ - step direction  \n",
                "\n",
                "Repeat until convergence means until some stopping rule is satisfied. The following are some of the commonly used stopping rules:  \n",
                "\n",
                "* *maxit* - setting a maximum number of iterations\n",
                "* $|\\beta^{(k+1)} - \\beta^{(k)}| < \\epsilon$ where $\\epsilon$ is very small(e.g $10^{-10}$)\n",
                "\n",
                "\n",
                "\n",
                "<!-- **Algorithm**   -->\n",
                "<!--   initial guess of $\\beta$   -->\n",
                "<!--     **repeat**   -->\n",
                "<!--       1. $\\Delta\\beta := -\\nabla RSS(\\beta)$ -->\n",
                "      \n",
                "\n",
                "\n",
                "## Closed-form vs GD\n",
                "\n",
                "In a lot of machine learning problems there are no closed-form solutions, therefore we have to use iterative optimization methods like Gradient Descent. In practice, when the data is high-dimensional it is much more efficient computationally to use iterative methods. With iterative methods it can be very challenging to find the right step-size because if it is too small- the descent might be very slow and if it is too large - gradient descent may fail to converge. \n",
                "\n",
                "## Model Assessment\n",
                "\n",
                "We need a way to assess how well the model fits the data. We typically use the $R^2$ and $RSE$ - *Residual Standard Error*\n",
                "\n",
                "$$RSE = \\sqrt{\\frac{1}{n-2}RSS}$$  \n",
                "If the model predictions are close to the true response values we will obtain a small RSE, which means our model fits the data.  \n",
                "\n",
                "How small is small enough for RSE? The answer depends on the data, but a better way to measure fit would be the $R^2$ since it is always between 0 and 1.  \n",
                "\n",
                "$$R^2 = \\frac{TSS-RSS}{TSS} = 1- \\frac{RSS}{TSS}$$\n",
                "$TSS = \\sum_{j=1}^{m} (y-\\bar y)^2 = (y- \\bar y)^T(y- \\bar y)$ - *Total Sum of Squares*, where $\\bar y$ is the mean of the observations. Recall that $RSS = (y-x\\beta)^T(y-x\\beta)$.  \n",
                "TSS measures the total variance in the response Y , and can be\n",
                " thought of as the amount of variability inherent in the response before the\n",
                "regression is performed. In contrast, RSS measures the amount of variability\n",
                "that is left unexplained after performing the regression.^[Gareth James et al An Introduction to Statistical Learning] \n",
                "\n",
                "<!-- ## Diagnostics -->\n",
                "\n",
                "# Implementation from scratch\n",
                "\n",
                "Here we will implement what we have discussed above from scratch in Python. \n",
                "<!-- and R. There are standard libraries/packages for this in both R and Python, we'll see how to use them too. -->\n",
                "\n",
                "## Simulating the data\n",
                "\n",
                "We will simulate a large dataset - $100000 \\times 10$. Below is a function in python to simulate linear regression data for any number of samples and predictors.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Simulation in Python\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "np.random.seed(1)\n",
                "\n",
                "def simulateLinearRegressionData(m, n):\n",
                "    \"\"\"\n",
                "  m - number of samples\n",
                "  n - number of features\n",
                "  \n",
                "  returns:\n",
                "    matrix X (m*n-1)\n",
                "    vector y(m)\n",
                "    vector beta(n)\n",
                "    \n",
                "  note: for convenience 1 has been added to the last column of X\n",
                "    \"\"\"\n",
                "    X = np.zeros((m,n-1))\n",
                "    beta = np.zeros(n)\n",
                "    for i in range(n-1):\n",
                "      mu = np.random.randint(0, 150)\n",
                "      sigma = np.random.normal(20, 0.1)\n",
                "      X[:,i] = np.random.normal(mu, sigma, size=m)\n",
                "      beta[i] =  np.random.normal(mu, sigma)\n",
                "      \n",
                "    X = np.hstack((X, np.ones((m,1))))\n",
                "    beta[-1] = np.random.normal()\n",
                "    beta =  np.reshape(beta, (n, -1))\n",
                "    y = np.dot(X, beta)\n",
                "    \n",
                "    return X, y, beta\n",
                "    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y, true_beta = simulateLinearRegressionData(100000, 10)\n",
                "print(\"Snippet of X\\n\", X[:5,:5], \"\\n\\nSnippet of y\\n\", y[:5], \"\\n\\nSnippet of true beta\\n\", true_beta[:5])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Python Closed-form\n",
                "\n",
                "Recall that: $\\hat{\\beta} = (X^TX)^{-1}X^Ty$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from numpy.linalg import inv\n",
                "\n",
                "def estimateBetaLR(X, y):\n",
                "  return np.dot(inv(np.dot(X.T, X)), np.dot(X.T, y))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "estimated_beta = estimateBetaLR(X, y)\n",
                "print(\"estimated beta,   true beta\\n\", np.hstack((estimated_beta, true_beta)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see the estimated coefficients are approximately equal to the ones we used for simulation. \n",
                "Note: On real world data, we never know what the true $\\beta$ is.\n",
                "\n",
                "\n",
                "## Python Gradient Descent\n",
                "\n",
                "The the \"oracle\" function is just a fancy way of describing the gradient $\\nabla RSS$ because it determines the descent direction.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(2)\n",
                "\n",
                "def calc_rss(_beta, x, y):\n",
                "  return float((y-x.dot(_beta)).T.dot(y-x.dot(_beta)))\n",
                "  \n",
                "  \n",
                "def oracle(_beta, x, y):\n",
                "  return -2 * np.dot(x.T, (y-np.dot(x,_beta)))\n",
                "  # return (-2*(x.T)).dot(y-x.dot(_beta))\n",
                "  \n",
                "  \n",
                "def gd(x, y, maxit, step_size):\n",
                "  beta = np.random.normal(size=(x.shape[1], 1))\n",
                "  rss_history = []\n",
                "  for i in range(maxit):\n",
                "    rss_history.append(calc_rss(beta, x, y))\n",
                "    step_direction = -oracle(beta, x, y)\n",
                "    beta = beta + step_size*step_direction\n",
                "    if i==maxit-1:\n",
                "      print(beta)\n",
                "    \n",
                "  return beta, rss_history\n",
                "    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gd_beta, rss_history = gd(X, y,50, 1e-10)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"gd-estimated beta,   true beta\\n\", np.hstack((gd_beta, true_beta)))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The gradient descent method provides a close approximation of the true \"beta\", but finding the right step-size is a very tedious task (this took me a while).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# library(reticulate)\n",
                "# py_install(\"matplotlib\")\n",
                "# py_install(\"--user loess\") \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below we can see that after the 20th iteration the method converges.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "plt.style.use(\"seaborn\")\n",
                "plt.plot(rss_history)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# mse = rss_history[-1]/(len(y)-2)\n",
                "# print(mse)\n",
                "# print(np.sqrt(748.0683431702506))\n",
                "# print(calc_rss(gd_beta,X, y ))\n",
                "# print(calc_rss(estimated_beta,X, y ))\n",
                "# print(np.hstack((estimated_beta, gd_beta)))\n",
                "# print(np.hstack(((y-X.dot(estimated_beta))[:10], (y-X.dot(gd_beta))[:10])))\n",
                "# print()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Python assessment\n",
                "\n",
                "Now that we have fit a linear regression to our data, we need a way to assess how well the model fits the data. We will use the $R^2$ and $RSE$. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tss(y):\n",
                "  return np.dot((y-np.mean(y)).T, ((y-np.mean(y))))\n",
                "  \n",
                "def rse_r2(_beta,x, y):\n",
                "  _rss = calc_rss(_beta,x, y )\n",
                "  _rse = np.sqrt(_rss/(len(y)-2))\n",
                "  _tss = tss(y)\n",
                "  _r2 = 1-(_rss/_tss)\n",
                "  return _rse, _r2\n",
                "  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rse, r2 = rse_r2(gd_beta, X, y)\n",
                "print(f\"RSE = {rse}\")\n",
                "print(f\"R2 = {float(r2)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Above we used the estimates of $\\beta$ obtained using gradient descent. An $R^2$ of ~ $0.985$ suggests $98.5\\%$ of the variability in the response(y) is explained by the regression. This means the linear regression model fits our data very well.\n",
                "\n",
                "\n",
                "## Python Diagnostics\n",
                "\n",
                "a.k.a residual analysis. Residual plots can help us identify non-linearity in the relationship between the response and predictors. The residual plot in the case of a single predictor is $y-\\hat y$ vs $x$, and for multiple predictors it is $y-\\hat y$ vs $\\hat y$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_residuals(x, y, _beta):\n",
                "  y_hat = x.dot(_beta)\n",
                "  e = y - y_hat\n",
                "  if x.shape[1] > 1:\n",
                "    plt.scatter(y_hat, e)\n",
                "    plt.show()\n",
                "  else:\n",
                "    plt.scatter(e, x[:,0])\n",
                "    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(20,10))\n",
                "plot_residuals(X, y, gd_beta)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There doesn't appear to be any strong pattern in the residuals which means the relationship in our data is linear. But in cases where there is a non-linear relationship in the data, we will get something like the one below. In the case of non-linear relationship we can use the non-linear transformations of the predictor variables or consider alternative non-linear methods.\n",
                "\n",
                "![Credit - Gareth James et al - Introduction to Statistical Learning](non_linear.png)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "X, y, true_beta = simulateLinearRegressionData(100000, 10)\n",
                "df_k = np.hstack((X[:,:-1], y))\n",
                "colnames = [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"y\"]\n",
                "df_k = pd.DataFrame(df_k, columns=colnames)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_k.to_csv(\"sim_data.csv\", index=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<!-- ## R Closed-form  -->\n",
                "\n",
                "\n",
                "\n",
                "<!-- ## R Gradient Descent -->\n",
                "\n",
                "<!-- ## R assessment -->\n",
                "\n",
                "<!-- ## R Diagnostics -->\n",
                "\n",
                "<!-- # Using Libraries -->\n",
                "\n",
                "<!-- ## Scikit-Learn -->\n",
                "\n",
                "<!-- ## R-libraries -->\n",
                "\n",
                "\n",
                "<!-- # Comments and Questions -->\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
